{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the Vaani Demo Notebook.\n",
    "#### Please make sure you have all the libraries installed (refer to `requirements.txt` document for help)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 12:59:50.611481: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 12:59:53.154265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 12:59:53.196819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 12:59:53.196901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "MAX_LENGTH = 256\n",
    "# CUDA suport test\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing word2vec model\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 13:00:01.369029: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 13:00:01.369933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 13:00:01.369982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 13:00:01.370020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 13:00:01.370469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 13:00:01.370522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 13:00:01.370532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-31 13:00:01.370558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-31 13:00:01.370593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4839 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# initializing all background prep that goes into prediction (open this cell for autopsy)\n",
    "# loading the model\n",
    "demomodel = tf.keras.models.load_model('./models/bhagvadgita/BEST_max_pooled_autoencoder.h5')\n",
    "demoencoder = tf.keras.Model(demomodel.input, demomodel.layers[5].output)\n",
    "gitaDemo = pd.read_csv('../dataset/bhagvadgita_encoded.csv')\n",
    "gitaDemo = gitaDemo.drop(['Unnamed: 0'], axis=1)\n",
    "gitaDemo = gitaDemo.drop([546], axis=0)\n",
    "gitaDemo['nlp'] = [nlp(i) for i in gitaDemo['English Translation']]\n",
    "\n",
    "# function definitions\n",
    "def tensorgoBrr(df: pd.DataFrame, column: str):\n",
    "    \"\"\"this function returns tensors for max pooled, mean pooled, and all present (padded up or down to 256 vectors)\n",
    "    the return is a tuple with the order max, mean, and all tokens respectively.\"\"\"\n",
    "    maxtokens = []\n",
    "    meantokens = []\n",
    "    alltokens = []\n",
    "\n",
    "    for i in df[column]:\n",
    "        doc = nlp(i)\n",
    "        tokenlist = np.array([token.vector.get() for token in doc])\n",
    "        maxnormlist = np.array([float(token.vector_norm) for token in doc])\n",
    "        if MAX_LENGTH - tokenlist.shape[0] > 0:\n",
    "            constructarr = np.zeros((MAX_LENGTH - tokenlist.shape[0], 300))\n",
    "            alltokens.append(np.append(tokenlist, constructarr, axis=(0)))\n",
    "        else:\n",
    "            alltokens.append(tokenlist[:MAX_LENGTH])\n",
    "\n",
    "        maxtokens.append(np.array(tokenlist[maxnormlist.argmax()]))\n",
    "        meantokens.append(np.array(tokenlist.mean(axis=(0))))\n",
    "\n",
    "    tmaxtokens = tf.convert_to_tensor(maxtokens)\n",
    "    tmeantokens = tf.convert_to_tensor(meantokens)\n",
    "    talltokens = tf.convert_to_tensor(alltokens)\n",
    "\n",
    "    return (tmaxtokens, tmeantokens, talltokens)\n",
    "\n",
    "def find_closest_gita_verse(token):\n",
    "    start = time.time()\n",
    "    # print(\"Let me Think\", end='')\n",
    "    # time.sleep(2)\n",
    "    # print('.',end='')\n",
    "    encoding = demoencoder.predict(token, verbose=0)\n",
    "    # time.sleep(0.4)\n",
    "    vectorComp = gitaDemo.iloc[:, -6:-1].to_numpy()\n",
    "    # print('.',end='')\n",
    "    # time.sleep(0.5)\n",
    "    # similarity rating function\n",
    "    diff = np.array([np.dot(encoding, i)/(np.linalg.norm(encoding)*np.linalg.norm(i)) for i in vectorComp])\n",
    "    seriesSlice = gitaDemo.iloc[diff.argmax(), 1:4]\n",
    "    #print('.')\n",
    "    # time.sleep(0.83)\n",
    "    # return a nicely formatted string.\n",
    "    exect = time.time() - start\n",
    "    return f\"AM4's Output: Chapter {seriesSlice[0]}, Verse {seriesSlice[1]} of the Bhagvadgita, which says:\\n\\n{seriesSlice[2]}\\nTime Taken: {exect}\"\n",
    "\n",
    "def find_spacy_output(text):\n",
    "    start = time.time()\n",
    "    vector = nlp(text)\n",
    "    simArr = np.array([vector.similarity(i) for i in gitaDemo['nlp']])\n",
    "    seriesSlice = gitaDemo.iloc[simArr.argmax(), 1:4]\n",
    "    exect = time.time() - start\n",
    "    return f\"Base Vector Comparison Output: Chapter {seriesSlice[0]}, Verse {seriesSlice[1]} of the Bhagvadgita, which says:\\n\\n{seriesSlice[2]}\\nTime Taken: {exect}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the cells, but rerun this one to experience the demo again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input: how should criminals be treated?\n",
      "---------------------------------\n",
      "AM4's Output: Chapter 3, Verse 33 of the Bhagvadgita, which says:\n",
      "\n",
      "All beings, wise or unwise, are forced to act by nature. What can restraint possibly do, O Arjuna?\n",
      "Time Taken: 0.054961442947387695\n",
      "---------------------------------\n",
      "Base Vector Comparison Output: Chapter 10, Verse 39 of the Bhagvadgita, which says:\n",
      "\n",
      "Arjuna, know that I am the seed of all things that are, and that no being that moves or moves not can ever be without Me.\n",
      "Time Taken: 0.3691697120666504\n"
     ]
    }
   ],
   "source": [
    "# DEMO CELL\n",
    "text = input('What Troubles you, friend?')\n",
    "print(f'Text Input: {text}' + '\\n---------------------------------')\n",
    "token, _, _ = tensorgoBrr(pd.DataFrame([text], columns=(['text'])), 'text')\n",
    "print(find_closest_gita_verse(token) + '\\n---------------------------------')\n",
    "print(find_spacy_output(text) + '\\n---------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for your time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
