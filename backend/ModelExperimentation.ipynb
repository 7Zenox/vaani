{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is developed as a documentation for experimentation and development of a model for the sacred.ai project created by Vasu Jain, Amee Madhani, Anand Chauhan, and Ananya Chauhan as part of our university course. The project to create one or multiple model in order to answer question based on life, the universe, and everything with the knowledge of several religious texts for Hinduism, Christianity, and Islam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:45:47.350867: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:45:49.530671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:45:49.601297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:45:49.601377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# verifying if cuda setup works properly\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current working approaches include looking to religious semantic searching and question-answering. The latter of these requires an extensive dataset covering a wide variety of questions and associated answers and outputs we'd be looking for, and is therefore hard to implement since we do not know if anything like that even exists (hopefully not lol)\n",
    "\n",
    "So we'd have to play around a lot with feature extraction, semantics and transformers and their encoding.e\n",
    "\n",
    "Can't do regression based linear transformations either since that, too, requires a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just import all our datasets for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Verse</th>\n",
       "      <th>English Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.1</td>\n",
       "      <td>Dhrtarashtra asked of Sanjaya: O SANJAYA, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.2</td>\n",
       "      <td>Sanjaya explained: Now seeing that the army of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.3</td>\n",
       "      <td>Behold O, Master, the mighty army of the sons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.4</td>\n",
       "      <td>Present here are the mighty archers, peers or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>Chapter 1</td>\n",
       "      <td>Verse 1.5</td>\n",
       "      <td>Dhrishtaketu, Chekitana, and the valiant king ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Title    Chapter      Verse  \\\n",
       "0  Arjuna's Vishada Yoga  Chapter 1  Verse 1.1   \n",
       "1  Arjuna's Vishada Yoga  Chapter 1  Verse 1.2   \n",
       "2  Arjuna's Vishada Yoga  Chapter 1  Verse 1.3   \n",
       "3  Arjuna's Vishada Yoga  Chapter 1  Verse 1.4   \n",
       "4  Arjuna's Vishada Yoga  Chapter 1  Verse 1.5   \n",
       "\n",
       "                                 English Translation  \n",
       "0  Dhrtarashtra asked of Sanjaya: O SANJAYA, what...  \n",
       "1  Sanjaya explained: Now seeing that the army of...  \n",
       "2  Behold O, Master, the mighty army of the sons ...  \n",
       "3  Present here are the mighty archers, peers or ...  \n",
       "4  Dhrishtaketu, Chekitana, and the valiant king ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhagvadgita = pd.read_csv('../dataset/gitaDataset.csv')\n",
    "quran = pd.read_csv('../dataset/quranDataset.csv')\n",
    "bible = pd.read_csv('../dataset/bibleDataset.csv')\n",
    "print(bhagvadgita.shape)\n",
    "bhagvadgita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Arjuna's Vishada Yoga\", 'Sankhya Yoga', 'Karma Yoga',\n",
       "       'Jnana-Karma-Sanyasa Yoga', 'Atma-Samyama Yoga',\n",
       "       'Jnana-Vijnana Yoga', 'Aksara-ParaBrahma Yoga',\n",
       "       'Raja-Vidya-Raja-Guhya Yoga', 'Vibhuti Yoga',\n",
       "       'Viswarupa-Darsana Yoga', 'Bhakti Yoga',\n",
       "       'Ksetra-Ksetrajna-Vibhaga Yoga', 'Gunatraya-Vibhaga Yoga',\n",
       "       'Purushottama Yoga', 'Daivasura-Sampad-Vibhaga Yoga',\n",
       "       'Shraddhatraya-Vibhaga Yoga', 'Moksha-Sanyasa Yoga'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhagvadgita['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6236, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Surah</th>\n",
       "      <th>Ayat</th>\n",
       "      <th>Verse</th>\n",
       "      <th>Tafseer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the name of Allah, the Beneficent, the Merc...</td>\n",
       "      <td>In the Name of God the Compassionate the Merciful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Praise be to Allah, Lord of the Worlds,</td>\n",
       "      <td>In the Name of God the name of a thing is that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>The Beneficent, the Merciful.</td>\n",
       "      <td>The Compassionate the Merciful that is to say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Owner of the Day of Judgment,</td>\n",
       "      <td>Master of the Day of Judgement that is the day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Opening</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Thee (alone) we worship; Thee (alone) we ask f...</td>\n",
       "      <td>You alone we worship and You alone we ask for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name  Surah  Ayat  \\\n",
       "0  The Opening      1     1   \n",
       "1  The Opening      1     2   \n",
       "2  The Opening      1     3   \n",
       "3  The Opening      1     4   \n",
       "4  The Opening      1     5   \n",
       "\n",
       "                                               Verse  \\\n",
       "0  In the name of Allah, the Beneficent, the Merc...   \n",
       "1            Praise be to Allah, Lord of the Worlds,   \n",
       "2                      The Beneficent, the Merciful.   \n",
       "3                      Owner of the Day of Judgment,   \n",
       "4  Thee (alone) we worship; Thee (alone) we ask f...   \n",
       "\n",
       "                                             Tafseer  \n",
       "0  In the Name of God the Compassionate the Merciful  \n",
       "1  In the Name of God the name of a thing is that...  \n",
       "2  The Compassionate the Merciful that is to say ...  \n",
       "3  Master of the Day of Judgement that is the day...  \n",
       "4  You alone we worship and You alone we ask for ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(quran.shape)\n",
    "quran.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31103, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>At the first God made the heaven and the earth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>And the earth was waste and without form; and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>And God, looking on the light, saw that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Naming the light, Day, and the dark, Night. An...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     field  book  chapter  verse  \\\n",
       "0  1001001     1        1      1   \n",
       "1  1001002     1        1      2   \n",
       "2  1001003     1        1      3   \n",
       "3  1001004     1        1      4   \n",
       "4  1001005     1        1      5   \n",
       "\n",
       "                                                text  \n",
       "0    At the first God made the heaven and the earth.  \n",
       "1  And the earth was waste and without form; and ...  \n",
       "2  And God said, Let there be light: and there wa...  \n",
       "3  And God, looking on the light, saw that it was...  \n",
       "4  Naming the light, Day, and the dark, Night. An...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bible.shape)\n",
    "bible.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title                  object\n",
       "Chapter                 int64\n",
       "Verse                   int64\n",
       "English Translation    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whatever approach we implement now we will implement first on the bhagvadgita, since it is the smallest.\n",
    "# everything else will be taken care of later lmao\n",
    "try:\n",
    "    # removing useless text and making everthing integer.\n",
    "    bhagvadgita['Verse'] = bhagvadgita['Verse'].apply(lambda x: x.split('.')[-1]).astype('int')\n",
    "    bhagvadgita['Chapter'] = bhagvadgita['Chapter'].apply(lambda x: x.split()[-1]).astype('int')\n",
    "except AttributeError:\n",
    "    # circumventing inplace problems\n",
    "    pass\n",
    "# check dtypes\n",
    "bhagvadgita.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "118\n",
      "273\n",
      "584\n"
     ]
    }
   ],
   "source": [
    "# finding the maximum amonut of words in any single shlok/verse/passage. we will model our max query length around it. maybe.\n",
    "# every book will therfore contain three tensors. max pooled, mean pooled, and all vectors with a lot of zeroes.\n",
    "max_length = 0\n",
    "for i in bhagvadgita['English Translation']:\n",
    "    if len(i.split()) > max_length:\n",
    "        max_length = len(i.split())\n",
    "print(max_length)\n",
    "for j in bible['text']:\n",
    "    if len(j.split()) > max_length:\n",
    "        max_length = len(j.split())\n",
    "print(max_length)\n",
    "for k in quran['Verse']:\n",
    "    if len(k.split()) > max_length:\n",
    "        max_length = len(k.split())\n",
    "print(max_length)\n",
    "for l in quran['Tafseer'].astype('str'):\n",
    "    if len(l.split()) > max_length:\n",
    "        max_length = len(l.split())\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 584 \n",
    "# best case, let's try smaller sizes first\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the word2vec model that we'll be using for now.\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max v/s overall vector: False \n",
      "Mean v/s overall vector: True \n",
      "Max v/s Argmax vector: False\n"
     ]
    }
   ],
   "source": [
    "# investigating if the standard vector function does max or mean pooling.\n",
    "lmaobase = np.array([token.vector.get() for token in nlp(bhagvadgita['English Translation'][0])])\n",
    "lmao = lmaobase.mean(axis=(0))\n",
    "argm = np.array([float(token.vector_norm) for token in nlp(bhagvadgita['English Translation'][0])])\n",
    "lmao1 = lmaobase[argm.argmax()]\n",
    "lmao2 = np.array(nlp(bhagvadgita['English Translation'][0]).vector.get())\n",
    "lmao3 = lmaobase.max(axis=(0))\n",
    "print(f\"Max v/s overall vector: {np.array_equal(lmao1, lmao2)} \\nMean v/s overall vector: {np.array_equal(lmao, lmao2)} \\nMax v/s Argmax vector: {np.array_equal(lmao1, lmao3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experimenting with how to make a vector with all zeroes\n",
    "# find a zero vector\n",
    "zerosample = np.array(nlp(bhagvadgita['English Translation'][0].split()[0]).vector.get())\n",
    "constructedzeros = np.zeros((300,))\n",
    "np.array_equal(constructedzeros, zerosample)\n",
    "zerosample = np.array([token.vector.get() for token in nlp(bhagvadgita['English Translation'][0])])\n",
    "constructarr = np.zeros((MAX_LENGTH - zerosample.shape[0], 300))\n",
    "hope = np.append(zerosample, constructarr, axis=(0))\n",
    "hope.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT WORKS!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can see that spacy's `en_core_web_lg` uses mean pooling. so we can make our data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:46:40.547638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-23 00:46:40.552152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:46:40.552257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:46:40.552286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:46:40.553450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:46:40.553495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:46:40.553503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-23 00:46:40.553530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-23 00:46:40.553575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3034 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-03-23 00:46:46.516051: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 430080000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([700, 300]),\n",
       " TensorShape([700, 300]),\n",
       " TensorShape([700, 256, 300]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with one field, trying both max and mean pooling.\n",
    "maxtokens = []\n",
    "meantokens = []\n",
    "alltokens = []\n",
    "\n",
    "for i in bhagvadgita['English Translation']:\n",
    "    doc = nlp(i)\n",
    "    tokenlist = np.array([token.vector.get() for token in doc])\n",
    "    maxnormlist = np.array([float(token.vector_norm) for token in doc])\n",
    "    if MAX_LENGTH - tokenlist.shape[0] > 0:\n",
    "        constructarr = np.zeros((MAX_LENGTH - tokenlist.shape[0], 300))\n",
    "        alltokens.append(np.append(tokenlist, constructarr, axis=(0)))\n",
    "    else:\n",
    "        alltokens.append(tokenlist[:MAX_LENGTH])\n",
    "\n",
    "    maxtokens.append(np.array(tokenlist[maxnormlist.argmax()]))\n",
    "    meantokens.append(np.array(tokenlist.mean(axis=(0))))\n",
    "\n",
    "tmaxtokens = tf.convert_to_tensor(maxtokens)\n",
    "tmeantokens = tf.convert_to_tensor(meantokens)\n",
    "talltokens = tf.convert_to_tensor(alltokens)\n",
    "\n",
    "tmaxtokens.shape, tmeantokens.shape, talltokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting this to a function for later use:\n",
    "def tensorgoBrr(df: pd.DataFrame, column: str):\n",
    "    \"\"\"this function returns tensors for max pooled, mean pooled, and all present (padded up or down to 256 vectors)\n",
    "    the return is a tuple with the order max, mean, and all tokens respectively.\"\"\"\n",
    "    maxtokens = []\n",
    "    meantokens = []\n",
    "    alltokens = []\n",
    "\n",
    "    for i in df[column]:\n",
    "        doc = nlp(i)\n",
    "        tokenlist = np.array([token.vector.get() for token in doc])\n",
    "        maxnormlist = np.array([float(token.vector_norm) for token in doc])\n",
    "        if MAX_LENGTH - tokenlist.shape[0] > 0:\n",
    "            constructarr = np.zeros((MAX_LENGTH - tokenlist.shape[0], 300))\n",
    "            alltokens.append(np.append(tokenlist, constructarr, axis=(0)))\n",
    "        else:\n",
    "            alltokens.append(tokenlist[:MAX_LENGTH])\n",
    "\n",
    "        maxtokens.append(np.array(tokenlist[maxnormlist.argmax()]))\n",
    "        meantokens.append(np.array(tokenlist.mean(axis=(0))))\n",
    "\n",
    "    tmaxtokens = tf.convert_to_tensor(maxtokens)\n",
    "    tmeantokens = tf.convert_to_tensor(meantokens)\n",
    "    talltokens = tf.convert_to_tensor(alltokens)\n",
    "\n",
    "    return (tmaxtokens, tmeantokens, talltokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created the tensors for all verses of the bhagvadgita. for the demo this is all that we wil be doing.\n",
    "\n",
    "Bear in mind that the tensor with all adjusted vectors for the Bhagvadgita alone is over 400 MB, which, considering my system VRAM of 8GB, might become a computational power problem later down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all AI experimentation libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed, Reshape, Flatten\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 300)]        0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 256, 60)           90060     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 256, 20)           3620      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 256, 10)           610       \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 256, 20)           620       \n",
      "                                                                 \n",
      " up_sampling1d (UpSampling1D  (None, 256, 20)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 256, 60)           3660      \n",
      "                                                                 \n",
      " up_sampling1d_1 (UpSampling  (None, 256, 60)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 256, 300)          18300     \n",
      "                                                                 \n",
      " up_sampling1d_2 (UpSampling  (None, 256, 300)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 256, 300)          450300    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 567,170\n",
      "Trainable params: 567,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "timesteps = talltokens.shape[1]  # Length of your sequences\n",
    "input_dim = talltokens.shape[2]\n",
    "latent_dim = 64\n",
    "\n",
    "input_img = keras.Input(shape=(256, 300))\n",
    "\n",
    "x = layers.Conv1D(300/5, (5), activation='tanh', padding='same')(input_img)\n",
    "# x = layers.MaxPooling1D((8), padding='same')(x)\n",
    "x = layers.Conv1D(60/3, (3), activation='sigmoid', padding='same')(x)\n",
    "# x = layers.MaxPooling1D((8), padding='same')(x)\n",
    "x = layers.Conv1D(20/2, (3), activation='relu', padding='same')(x)\n",
    "# encoded = layers.MaxPooling1D((4), padding='same')(x)\n",
    "encoded = x\n",
    "\n",
    "x = layers.Conv1D(20, (3), activation='tanh', padding='same')(encoded)\n",
    "x = layers.UpSampling1D((1))(x)\n",
    "x = layers.Conv1D(60, (3), activation='sigmoid', padding='same')(x)\n",
    "x = layers.UpSampling1D((1))(x)\n",
    "x = layers.Conv1D(300, (1), activation='sigmoid')(x)\n",
    "x = layers.UpSampling1D((1))(x)\n",
    "decoded = layers.Conv1D(300, (5), activation='relu', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss=['cosine_similarity'], metrics=['mse', 'cosine_similarity'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:46:50.210460: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 430080000 exceeds 10% of free system memory.\n",
      "2023-03-23 00:46:52.540006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-03-23 00:46:54.287036: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-03-23 00:46:54.744058: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fee9291f960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-23 00:46:54.744107: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 Super, Compute Capability 7.5\n",
      "2023-03-23 00:46:54.752169: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-23 00:46:54.980905: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-03-23 00:46:55.024556: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 12s 13ms/step - loss: -0.0513 - mse: 6.0704 - cosine_similarity: 0.0513\n",
      "Epoch 2/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0607 - mse: 3.5921 - cosine_similarity: 0.0607\n",
      "Epoch 3/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0645 - mse: 3.3468 - cosine_similarity: 0.0645\n",
      "Epoch 4/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0674 - mse: 3.3237 - cosine_similarity: 0.0674\n",
      "Epoch 5/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0702 - mse: 3.3187 - cosine_similarity: 0.0702\n",
      "Epoch 6/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0729 - mse: 3.3720 - cosine_similarity: 0.0729\n",
      "Epoch 7/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0746 - mse: 3.3682 - cosine_similarity: 0.0746\n",
      "Epoch 8/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0758 - mse: 3.3709 - cosine_similarity: 0.0758\n",
      "Epoch 9/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0776 - mse: 3.3711 - cosine_similarity: 0.0776\n",
      "Epoch 10/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0786 - mse: 3.3784 - cosine_similarity: 0.0786\n",
      "Epoch 11/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0797 - mse: 3.3466 - cosine_similarity: 0.0797\n",
      "Epoch 12/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0808 - mse: 3.3496 - cosine_similarity: 0.0808\n",
      "Epoch 13/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0818 - mse: 3.3562 - cosine_similarity: 0.0818\n",
      "Epoch 14/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0833 - mse: 3.3570 - cosine_similarity: 0.0833\n",
      "Epoch 15/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0840 - mse: 3.3533 - cosine_similarity: 0.0840\n",
      "Epoch 16/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0846 - mse: 3.3832 - cosine_similarity: 0.0846\n",
      "Epoch 17/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0851 - mse: 3.4179 - cosine_similarity: 0.0851\n",
      "Epoch 18/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0857 - mse: 3.4544 - cosine_similarity: 0.0857\n",
      "Epoch 19/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0861 - mse: 3.4700 - cosine_similarity: 0.0861\n",
      "Epoch 20/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0866 - mse: 3.4256 - cosine_similarity: 0.0866\n",
      "Epoch 21/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0874 - mse: 3.4213 - cosine_similarity: 0.0874\n",
      "Epoch 22/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0879 - mse: 3.4403 - cosine_similarity: 0.0879\n",
      "Epoch 23/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0884 - mse: 3.4673 - cosine_similarity: 0.0884\n",
      "Epoch 24/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0888 - mse: 3.5366 - cosine_similarity: 0.0888\n",
      "Epoch 25/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0892 - mse: 3.5114 - cosine_similarity: 0.0892\n",
      "Epoch 26/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0895 - mse: 3.5072 - cosine_similarity: 0.0895\n",
      "Epoch 27/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0899 - mse: 3.4785 - cosine_similarity: 0.0899\n",
      "Epoch 28/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0902 - mse: 3.5099 - cosine_similarity: 0.0902\n",
      "Epoch 29/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0906 - mse: 3.4129 - cosine_similarity: 0.0906\n",
      "Epoch 30/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0909 - mse: 3.3381 - cosine_similarity: 0.0909\n",
      "Epoch 31/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0912 - mse: 3.3165 - cosine_similarity: 0.0912\n",
      "Epoch 32/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0916 - mse: 3.3296 - cosine_similarity: 0.0916\n",
      "Epoch 33/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0919 - mse: 3.3321 - cosine_similarity: 0.0919\n",
      "Epoch 34/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0922 - mse: 3.3391 - cosine_similarity: 0.0922\n",
      "Epoch 35/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0924 - mse: 3.3546 - cosine_similarity: 0.0924\n",
      "Epoch 36/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0928 - mse: 3.3466 - cosine_similarity: 0.0928\n",
      "Epoch 37/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0930 - mse: 3.3347 - cosine_similarity: 0.0930\n",
      "Epoch 38/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0932 - mse: 3.3316 - cosine_similarity: 0.0932\n",
      "Epoch 39/75\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.0935 - mse: 3.3354 - cosine_similarity: 0.0935\n",
      "Epoch 40/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0937 - mse: 3.4214 - cosine_similarity: 0.0937\n",
      "Epoch 41/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0939 - mse: 3.3629 - cosine_similarity: 0.0939\n",
      "Epoch 42/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0941 - mse: 3.3935 - cosine_similarity: 0.0941\n",
      "Epoch 43/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0943 - mse: 3.3566 - cosine_similarity: 0.0943\n",
      "Epoch 44/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0944 - mse: 3.3927 - cosine_similarity: 0.0944\n",
      "Epoch 45/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0948 - mse: 3.3700 - cosine_similarity: 0.0948\n",
      "Epoch 46/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0950 - mse: 3.3536 - cosine_similarity: 0.0950\n",
      "Epoch 47/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0951 - mse: 3.4016 - cosine_similarity: 0.0951\n",
      "Epoch 48/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0953 - mse: 3.3767 - cosine_similarity: 0.0953\n",
      "Epoch 49/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0957 - mse: 3.3621 - cosine_similarity: 0.0957\n",
      "Epoch 50/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0959 - mse: 3.3710 - cosine_similarity: 0.0959\n",
      "Epoch 51/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0961 - mse: 3.3853 - cosine_similarity: 0.0961\n",
      "Epoch 52/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0963 - mse: 3.3532 - cosine_similarity: 0.0963\n",
      "Epoch 53/75\n",
      "350/350 [==============================] - 3s 10ms/step - loss: -0.0964 - mse: 3.3936 - cosine_similarity: 0.0964\n",
      "Epoch 54/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0969 - mse: 3.3500 - cosine_similarity: 0.0969\n",
      "Epoch 55/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0973 - mse: 3.3483 - cosine_similarity: 0.0973\n",
      "Epoch 56/75\n",
      "350/350 [==============================] - 4s 10ms/step - loss: -0.0974 - mse: 3.3484 - cosine_similarity: 0.0974\n",
      "Epoch 57/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0975 - mse: 3.2819 - cosine_similarity: 0.0975\n",
      "Epoch 58/75\n",
      "350/350 [==============================] - 5s 13ms/step - loss: -0.0977 - mse: 3.2914 - cosine_similarity: 0.0977\n",
      "Epoch 59/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0979 - mse: 3.3372 - cosine_similarity: 0.0979\n",
      "Epoch 60/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0980 - mse: 3.3314 - cosine_similarity: 0.0980\n",
      "Epoch 61/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0981 - mse: 3.2931 - cosine_similarity: 0.0981\n",
      "Epoch 62/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0983 - mse: 3.2873 - cosine_similarity: 0.0983\n",
      "Epoch 63/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0984 - mse: 3.2598 - cosine_similarity: 0.0984\n",
      "Epoch 64/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0986 - mse: 3.2564 - cosine_similarity: 0.0986\n",
      "Epoch 65/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0987 - mse: 3.2611 - cosine_similarity: 0.0987\n",
      "Epoch 66/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0989 - mse: 3.2370 - cosine_similarity: 0.0989\n",
      "Epoch 67/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0991 - mse: 3.2702 - cosine_similarity: 0.0991\n",
      "Epoch 68/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0992 - mse: 3.2573 - cosine_similarity: 0.0992\n",
      "Epoch 69/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0994 - mse: 3.2451 - cosine_similarity: 0.0994\n",
      "Epoch 70/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0996 - mse: 3.2356 - cosine_similarity: 0.0996\n",
      "Epoch 71/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0997 - mse: 3.2295 - cosine_similarity: 0.0997\n",
      "Epoch 72/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.0998 - mse: 3.2311 - cosine_similarity: 0.0998\n",
      "Epoch 73/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.0999 - mse: 3.2172 - cosine_similarity: 0.0999\n",
      "Epoch 74/75\n",
      "350/350 [==============================] - 4s 11ms/step - loss: -0.1001 - mse: 3.2125 - cosine_similarity: 0.1001\n",
      "Epoch 75/75\n",
      "350/350 [==============================] - 4s 12ms/step - loss: -0.1004 - mse: 3.2046 - cosine_similarity: 0.1004\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(talltokens, talltokens, batch_size=2, epochs=75, shuffle=True, verbose=1, callbacks=tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That model performance is unsatisfactory, with a cosine similarity of `~0.103`, it goes rather far from our input vector.\n",
    "WE will now try deep neural autoencoding instead, on all three of our tensors (max, mean, all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUT.\n",
    "our current hypothesis is that if we scale all our word embeddings like this for just compression's sake then maybe similarity won't matter when we try to match them.\n",
    "(in case this approach also does not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               45150     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 75)                11325     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                1900      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 130       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 25)                150       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 75)                1950      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 150)               11400     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 300)               45300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,305\n",
      "Trainable params: 117,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# trying again but with a deep neural network this time.\n",
    "input_imgmax = keras.Input(shape=(300))\n",
    "dencoded = Flatten()(input_imgmax)\n",
    "dencoded = Dense(units=300/2, activation='relu')(dencoded)\n",
    "dencoded = Dense(units=300/4, activation='relu')(dencoded)\n",
    "dencoded = Dense(units=300/12, activation='relu')(dencoded)\n",
    "dencoded = Dense(units=300/60, activation='relu')(dencoded)\n",
    "\n",
    "ddecoded = Dense(units=300/12, activation='relu')(dencoded)\n",
    "ddecoded = Dense(units=300/4, activation='relu')(ddecoded)\n",
    "ddecoded = Dense(units=300/2, activation='sigmoid')(ddecoded)\n",
    "ddecoded = Dense(units=300, activation='sigmoid')(ddecoded)\n",
    "\n",
    "dmaxautoencoder = keras.Model(input_imgmax, ddecoded)\n",
    "dmaxautoencoder.compile(optimizer='adam', loss=['cosine_similarity'], metrics=['mse', 'cosine_similarity'])\n",
    "dmaxautoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "350/350 [==============================] - 4s 6ms/step - loss: -0.3247 - mse: 57.1899 - cosine_similarity: 0.3247\n",
      "Epoch 2/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.4964 - mse: 57.5034 - cosine_similarity: 0.4964\n",
      "Epoch 3/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5491 - mse: 57.7563 - cosine_similarity: 0.5491\n",
      "Epoch 4/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5666 - mse: 57.7555 - cosine_similarity: 0.5666\n",
      "Epoch 5/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5910 - mse: 57.6627 - cosine_similarity: 0.5910\n",
      "Epoch 6/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6013 - mse: 57.7333 - cosine_similarity: 0.6013\n",
      "Epoch 7/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6090 - mse: 57.7385 - cosine_similarity: 0.6090\n",
      "Epoch 8/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5963 - mse: 57.7616 - cosine_similarity: 0.5963\n",
      "Epoch 9/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6109 - mse: 57.7042 - cosine_similarity: 0.6109\n",
      "Epoch 10/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6154 - mse: 57.6631 - cosine_similarity: 0.6154\n",
      "Epoch 11/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6149 - mse: 57.6075 - cosine_similarity: 0.6149\n",
      "Epoch 12/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6166 - mse: 57.5918 - cosine_similarity: 0.6166\n",
      "Epoch 13/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6188 - mse: 57.5960 - cosine_similarity: 0.6188\n",
      "Epoch 14/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6252 - mse: 57.6134 - cosine_similarity: 0.6252\n",
      "Epoch 15/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6277 - mse: 57.6719 - cosine_similarity: 0.6277\n",
      "Epoch 16/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6229 - mse: 57.6657 - cosine_similarity: 0.6229\n",
      "Epoch 17/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5974 - mse: 57.6497 - cosine_similarity: 0.5974\n",
      "Epoch 18/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6045 - mse: 57.5508 - cosine_similarity: 0.6045\n",
      "Epoch 19/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6017 - mse: 57.6013 - cosine_similarity: 0.6017\n",
      "Epoch 20/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5930 - mse: 57.6086 - cosine_similarity: 0.5930\n",
      "Epoch 21/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6019 - mse: 57.5744 - cosine_similarity: 0.6019\n",
      "Epoch 22/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6134 - mse: 57.4525 - cosine_similarity: 0.6134\n",
      "Epoch 23/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6123 - mse: 57.4365 - cosine_similarity: 0.6123\n",
      "Epoch 24/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6170 - mse: 57.4270 - cosine_similarity: 0.6170\n",
      "Epoch 25/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6067 - mse: 57.3579 - cosine_similarity: 0.6067\n",
      "Epoch 26/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6007 - mse: 57.3424 - cosine_similarity: 0.6007\n",
      "Epoch 27/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5999 - mse: 57.3882 - cosine_similarity: 0.5999\n",
      "Epoch 28/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6073 - mse: 57.3565 - cosine_similarity: 0.6073\n",
      "Epoch 29/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5896 - mse: 57.4164 - cosine_similarity: 0.5896\n",
      "Epoch 30/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5762 - mse: 57.6685 - cosine_similarity: 0.5762\n",
      "Epoch 31/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.5925 - mse: 57.5872 - cosine_similarity: 0.5925\n",
      "Epoch 32/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6024 - mse: 57.4463 - cosine_similarity: 0.6024\n",
      "Epoch 33/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6062 - mse: 57.4832 - cosine_similarity: 0.6062\n",
      "Epoch 34/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6091 - mse: 57.4369 - cosine_similarity: 0.6091\n",
      "Epoch 35/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6113 - mse: 57.4197 - cosine_similarity: 0.6113\n",
      "Epoch 36/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6117 - mse: 57.3429 - cosine_similarity: 0.6117\n",
      "Epoch 37/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6135 - mse: 57.3086 - cosine_similarity: 0.6135\n",
      "Epoch 38/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6141 - mse: 57.2759 - cosine_similarity: 0.6141\n",
      "Epoch 39/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6155 - mse: 57.2240 - cosine_similarity: 0.6155\n",
      "Epoch 40/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6155 - mse: 57.1965 - cosine_similarity: 0.6155\n",
      "Epoch 41/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6162 - mse: 57.1520 - cosine_similarity: 0.6162\n",
      "Epoch 42/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6164 - mse: 57.0707 - cosine_similarity: 0.6164\n",
      "Epoch 43/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6172 - mse: 57.0772 - cosine_similarity: 0.6172\n",
      "Epoch 44/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6169 - mse: 57.0571 - cosine_similarity: 0.6169\n",
      "Epoch 45/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6173 - mse: 57.0441 - cosine_similarity: 0.6173\n",
      "Epoch 46/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.5638 - mse: 57.0950 - cosine_similarity: 0.5638\n",
      "Epoch 47/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.5781 - mse: 57.1774 - cosine_similarity: 0.5781\n",
      "Epoch 48/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.5800 - mse: 57.2516 - cosine_similarity: 0.5800\n",
      "Epoch 49/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.5726 - mse: 57.0435 - cosine_similarity: 0.5726\n",
      "Epoch 50/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.5717 - mse: 56.9069 - cosine_similarity: 0.5717\n"
     ]
    }
   ],
   "source": [
    "history = dmaxautoencoder.fit(tmaxtokens, tmaxtokens, batch_size=2, epochs=50, shuffle=True, verbose=1, callbacks=tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is miles better, but the max pooling does mean there is a not insignificant loss of data here. will try flattening top 8 vectors and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "xdproc = layers.MaxPooling1D((32), padding='same')(input_img)\n",
    "xdp = keras.Model(input_img, xdproc)\n",
    "inpprep = xdp.predict(talltokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 8, 300)]          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2400)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1200)              2881200   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 150)               180150    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 75)                11325     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 25)                1900      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 5)                 130       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 25)                150       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 75)                1950      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 150)               11400     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 300)               45300     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 2400)              722400    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 300)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,855,905\n",
      "Trainable params: 3,855,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xdinput = keras.Input(shape=(8, 300))\n",
    "\n",
    "xdencoded = Flatten()(xdinput)\n",
    "xdencoded = Dense(units=300*4, activation='relu')(xdencoded)\n",
    "xdencoded = Dense(units=300/2, activation='relu')(xdencoded)\n",
    "xdencoded = Dense(units=300/4, activation='relu')(xdencoded)\n",
    "xdencoded = Dense(units=300/12, activation='relu')(xdencoded)\n",
    "xdencoded = Dense(units=300/60, activation='relu')(xdencoded)\n",
    "\n",
    "xddecoded = Dense(units=300/12, activation='relu')(xdencoded)\n",
    "xddecoded = Dense(units=300/4, activation='relu')(xddecoded)\n",
    "xddecoded = Dense(units=300/2, activation='sigmoid')(xddecoded)\n",
    "xddecoded = Dense(units=300, activation='sigmoid')(xddecoded)\n",
    "xddecoded = Dense(units=300*8, activation='relu')(xddecoded)\n",
    "xddecoded = Reshape((8, 300))(xddecoded)\n",
    "\n",
    "xdmaxautoencoder = keras.Model(xdinput, xddecoded)\n",
    "xdmaxautoencoder.compile(optimizer='adam', loss=['cosine_similarity'], metrics=['mse', 'cosine_similarity'])\n",
    "xdmaxautoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "349/350 [============================>.] - ETA: 0s - loss: -0.1697 - mse: 15.6377 - cosine_similarity: 0.1697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:53:21.796972: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 691200000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 6s 8ms/step - loss: -0.1700 - mse: 15.6471 - cosine_similarity: 0.1700\n",
      "Epoch 2/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1723 - mse: 14.6201 - cosine_similarity: 0.1723\n",
      "Epoch 3/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1737 - mse: 13.9579 - cosine_similarity: 0.1737\n",
      "Epoch 4/50\n",
      "347/350 [============================>.] - ETA: 0s - loss: -0.1741 - mse: 13.5398 - cosine_similarity: 0.1741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:53:29.394679: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 691200000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1741 - mse: 13.5475 - cosine_similarity: 0.1741\n",
      "Epoch 5/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1747 - mse: 13.2638 - cosine_similarity: 0.1747\n",
      "Epoch 6/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1748 - mse: 13.0566 - cosine_similarity: 0.1748\n",
      "Epoch 7/50\n",
      "346/350 [============================>.] - ETA: 0s - loss: -0.1753 - mse: 13.0048 - cosine_similarity: 0.1753"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 00:53:37.163099: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 691200000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 3s 9ms/step - loss: -0.1752 - mse: 13.0040 - cosine_similarity: 0.1752\n",
      "Epoch 8/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1756 - mse: 12.9342 - cosine_similarity: 0.1756\n",
      "Epoch 9/50\n",
      "350/350 [==============================] - 3s 7ms/step - loss: -0.1759 - mse: 12.9939 - cosine_similarity: 0.1759\n",
      "Epoch 10/50\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.1761 - mse: 13.0816 - cosine_similarity: 0.1761\n",
      "Epoch 11/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1762 - mse: 13.2833 - cosine_similarity: 0.1762\n",
      "Epoch 12/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1774 - mse: 13.4820 - cosine_similarity: 0.1774\n",
      "Epoch 13/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1786 - mse: 13.5378 - cosine_similarity: 0.1786\n",
      "Epoch 14/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1797 - mse: 13.7798 - cosine_similarity: 0.1797\n",
      "Epoch 15/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1807 - mse: 14.2622 - cosine_similarity: 0.1807\n",
      "Epoch 16/50\n",
      "350/350 [==============================] - 3s 7ms/step - loss: -0.1807 - mse: 14.7587 - cosine_similarity: 0.1807\n",
      "Epoch 17/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1807 - mse: 15.1807 - cosine_similarity: 0.1807\n",
      "Epoch 18/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1808 - mse: 15.6825 - cosine_similarity: 0.1808\n",
      "Epoch 19/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1808 - mse: 16.1821 - cosine_similarity: 0.1808\n",
      "Epoch 20/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1808 - mse: 16.6235 - cosine_similarity: 0.1808\n",
      "Epoch 21/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1811 - mse: 17.0818 - cosine_similarity: 0.1811\n",
      "Epoch 22/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1813 - mse: 17.5638 - cosine_similarity: 0.1813\n",
      "Epoch 23/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1820 - mse: 17.8510 - cosine_similarity: 0.1820\n",
      "Epoch 24/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1824 - mse: 18.3865 - cosine_similarity: 0.1824\n",
      "Epoch 25/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1828 - mse: 19.1017 - cosine_similarity: 0.1828\n",
      "Epoch 26/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1828 - mse: 19.7346 - cosine_similarity: 0.1828\n",
      "Epoch 27/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1828 - mse: 20.7076 - cosine_similarity: 0.1828\n",
      "Epoch 28/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1830 - mse: 21.1683 - cosine_similarity: 0.1830\n",
      "Epoch 29/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1832 - mse: 21.7667 - cosine_similarity: 0.1832\n",
      "Epoch 30/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1832 - mse: 22.4910 - cosine_similarity: 0.1832\n",
      "Epoch 31/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1832 - mse: 23.1907 - cosine_similarity: 0.1832\n",
      "Epoch 32/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1833 - mse: 23.8369 - cosine_similarity: 0.1833\n",
      "Epoch 33/50\n",
      "350/350 [==============================] - 3s 7ms/step - loss: -0.1833 - mse: 24.5266 - cosine_similarity: 0.1833\n",
      "Epoch 34/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1835 - mse: 24.9519 - cosine_similarity: 0.1835\n",
      "Epoch 35/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1838 - mse: 26.2048 - cosine_similarity: 0.1838\n",
      "Epoch 36/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1838 - mse: 26.4988 - cosine_similarity: 0.1838\n",
      "Epoch 37/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1839 - mse: 27.4012 - cosine_similarity: 0.1839\n",
      "Epoch 38/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1840 - mse: 27.9738 - cosine_similarity: 0.1840\n",
      "Epoch 39/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1840 - mse: 28.8023 - cosine_similarity: 0.1840\n",
      "Epoch 40/50\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.1840 - mse: 29.4437 - cosine_similarity: 0.1840\n",
      "Epoch 41/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1843 - mse: 30.1344 - cosine_similarity: 0.1843\n",
      "Epoch 42/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1844 - mse: 30.7416 - cosine_similarity: 0.1844\n",
      "Epoch 43/50\n",
      "350/350 [==============================] - 3s 9ms/step - loss: -0.1844 - mse: 31.5313 - cosine_similarity: 0.1844\n",
      "Epoch 44/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1845 - mse: 32.3078 - cosine_similarity: 0.1845\n",
      "Epoch 45/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.1845 - mse: 32.8264 - cosine_similarity: 0.1845\n",
      "Epoch 46/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1847 - mse: 33.5607 - cosine_similarity: 0.1847\n",
      "Epoch 47/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1847 - mse: 34.3433 - cosine_similarity: 0.1847\n",
      "Epoch 48/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1846 - mse: 35.1157 - cosine_similarity: 0.1846\n",
      "Epoch 49/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.1847 - mse: 35.6954 - cosine_similarity: 0.1847\n",
      "Epoch 50/50\n",
      "350/350 [==============================] - 2s 7ms/step - loss: -0.1847 - mse: 36.6215 - cosine_similarity: 0.1847\n"
     ]
    }
   ],
   "source": [
    "history = xdmaxautoencoder.fit(inpprep, inpprep, batch_size=2, epochs=50, shuffle=True, verbose=1, callbacks=tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so so far the max pooled one has worked the best. i wonder if i can get better results with mean, so i will rebuild the same model with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               45150     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 75)                11325     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                1900      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 130       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 25)                150       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 75)                1950      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 150)               11400     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 300)               45300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,305\n",
      "Trainable params: 117,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dmeanautoencoder = keras.Model(input_imgmax, ddecoded)\n",
    "dmeanautoencoder.compile(optimizer='adam', loss=['cosine_similarity'], metrics=['mse', 'cosine_similarity'])\n",
    "dmeanautoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "350/350 [==============================] - 4s 6ms/step - loss: -0.5846 - mse: 2.7308 - cosine_similarity: 0.5846\n",
      "Epoch 2/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6147 - mse: 2.7922 - cosine_similarity: 0.6147\n",
      "Epoch 3/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6189 - mse: 2.8326 - cosine_similarity: 0.6189\n",
      "Epoch 4/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6200 - mse: 2.8565 - cosine_similarity: 0.6200\n",
      "Epoch 5/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6227 - mse: 2.8809 - cosine_similarity: 0.6227\n",
      "Epoch 6/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6274 - mse: 2.8689 - cosine_similarity: 0.6274\n",
      "Epoch 7/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6275 - mse: 2.8717 - cosine_similarity: 0.6275\n",
      "Epoch 8/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6259 - mse: 2.8710 - cosine_similarity: 0.6259\n",
      "Epoch 9/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6281 - mse: 2.8611 - cosine_similarity: 0.6281\n",
      "Epoch 10/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6272 - mse: 2.8728 - cosine_similarity: 0.6272\n",
      "Epoch 11/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6256 - mse: 2.8502 - cosine_similarity: 0.6256\n",
      "Epoch 12/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6272 - mse: 2.8699 - cosine_similarity: 0.6272\n",
      "Epoch 13/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6274 - mse: 2.8663 - cosine_similarity: 0.6274\n",
      "Epoch 14/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6268 - mse: 2.8566 - cosine_similarity: 0.6268\n",
      "Epoch 15/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6287 - mse: 2.8653 - cosine_similarity: 0.6287\n",
      "Epoch 16/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6282 - mse: 2.8720 - cosine_similarity: 0.6282\n",
      "Epoch 17/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6282 - mse: 2.8706 - cosine_similarity: 0.6282\n",
      "Epoch 18/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6284 - mse: 2.8580 - cosine_similarity: 0.6284\n",
      "Epoch 19/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6290 - mse: 2.8605 - cosine_similarity: 0.6290\n",
      "Epoch 20/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6293 - mse: 2.8687 - cosine_similarity: 0.6293\n",
      "Epoch 21/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6289 - mse: 2.8636 - cosine_similarity: 0.6289\n",
      "Epoch 22/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6286 - mse: 2.8437 - cosine_similarity: 0.6286\n",
      "Epoch 23/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6286 - mse: 2.8587 - cosine_similarity: 0.6286\n",
      "Epoch 24/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6296 - mse: 2.8711 - cosine_similarity: 0.6296\n",
      "Epoch 25/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6298 - mse: 2.8760 - cosine_similarity: 0.6298\n",
      "Epoch 26/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6244 - mse: 2.8925 - cosine_similarity: 0.6244\n",
      "Epoch 27/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6237 - mse: 2.8918 - cosine_similarity: 0.6237\n",
      "Epoch 28/50\n",
      "350/350 [==============================] - 3s 8ms/step - loss: -0.6241 - mse: 2.8900 - cosine_similarity: 0.6241\n",
      "Epoch 29/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6247 - mse: 2.8546 - cosine_similarity: 0.6247\n",
      "Epoch 30/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6244 - mse: 2.8872 - cosine_similarity: 0.6244\n",
      "Epoch 31/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6204 - mse: 2.8757 - cosine_similarity: 0.6204\n",
      "Epoch 32/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6276 - mse: 2.8317 - cosine_similarity: 0.6276\n",
      "Epoch 33/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6277 - mse: 2.8748 - cosine_similarity: 0.6277\n",
      "Epoch 34/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6294 - mse: 2.8790 - cosine_similarity: 0.6294\n",
      "Epoch 35/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6298 - mse: 2.8734 - cosine_similarity: 0.6298\n",
      "Epoch 36/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6262 - mse: 2.8918 - cosine_similarity: 0.6262\n",
      "Epoch 37/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6278 - mse: 2.8706 - cosine_similarity: 0.6278\n",
      "Epoch 38/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6242 - mse: 2.8714 - cosine_similarity: 0.6242\n",
      "Epoch 39/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6294 - mse: 2.8762 - cosine_similarity: 0.6294\n",
      "Epoch 40/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6298 - mse: 2.8725 - cosine_similarity: 0.6298\n",
      "Epoch 41/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6301 - mse: 2.8598 - cosine_similarity: 0.6301\n",
      "Epoch 42/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6297 - mse: 2.8738 - cosine_similarity: 0.6297\n",
      "Epoch 43/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6289 - mse: 2.8605 - cosine_similarity: 0.6289\n",
      "Epoch 44/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6271 - mse: 2.8434 - cosine_similarity: 0.6271\n",
      "Epoch 45/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6260 - mse: 2.8818 - cosine_similarity: 0.6260\n",
      "Epoch 46/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6253 - mse: 2.8839 - cosine_similarity: 0.6253\n",
      "Epoch 47/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6272 - mse: 2.8790 - cosine_similarity: 0.6272\n",
      "Epoch 48/50\n",
      "350/350 [==============================] - 2s 5ms/step - loss: -0.6283 - mse: 2.8788 - cosine_similarity: 0.6283\n",
      "Epoch 49/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6265 - mse: 2.8716 - cosine_similarity: 0.6265\n",
      "Epoch 50/50\n",
      "350/350 [==============================] - 2s 6ms/step - loss: -0.6285 - mse: 2.8773 - cosine_similarity: 0.6285\n"
     ]
    }
   ],
   "source": [
    "history = dmeanautoencoder.fit(tmeantokens, tmeantokens, batch_size=2, epochs=50, shuffle=True, verbose=1, callbacks=tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better, but because of best practice concerns, we _MIGHT_ be using the max autoencoder for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all the models, starting with the latest one\n",
    "dmeanautoencoder.save('./models/bhagvadgita/mean_pooled_autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the bad max pool model\n",
    "xdmaxautoencoder.save('./models/bhagvadgita/top_8_max_pooled_autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving teh good max pool model\n",
    "dmaxautoencoder.save('./models/bhagvadgita/BEST_max_pooled_autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the conv1d model\n",
    "autoencoder.save('./models/bhagvadgita/convolutional_autoencoder.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can use the encoded values and save them with the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the model into just the encoder\n",
    "ourencoder = keras.Model(dmeanautoencoder.input, dmeanautoencoder.layers[5].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "bhagvadgita_encodings = ourencoder.predict(tmeantokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1082.754639</td>\n",
       "      <td>896.849670</td>\n",
       "      <td>642.481567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>723.449585</td>\n",
       "      <td>1098.451416</td>\n",
       "      <td>427.873535</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1376.264160</td>\n",
       "      <td>1331.519897</td>\n",
       "      <td>2368.946045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1003.821289</td>\n",
       "      <td>1086.919678</td>\n",
       "      <td>1332.740845</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.724609</td>\n",
       "      <td>1457.762939</td>\n",
       "      <td>1932.723999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            a1           a2           a3           a4   a5\n",
       "0  1082.754639   896.849670   642.481567     0.000000  0.0\n",
       "1     0.000000   723.449585  1098.451416   427.873535  0.0\n",
       "2     0.000000  1376.264160  1331.519897  2368.946045  0.0\n",
       "3     0.000000  1003.821289  1086.919678  1332.740845  0.0\n",
       "4     0.000000  1400.724609  1457.762939  1932.723999  0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodf = pd.DataFrame(bhagvadgita_encodings, columns=('a1','a2','a3','a4','a5'))\n",
    "encodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Verse</th>\n",
       "      <th>English Translation</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dhrtarashtra asked of Sanjaya: O SANJAYA, what...</td>\n",
       "      <td>1082.754639</td>\n",
       "      <td>896.849670</td>\n",
       "      <td>642.481567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Sanjaya explained: Now seeing that the army of...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>723.449585</td>\n",
       "      <td>1098.451416</td>\n",
       "      <td>427.873535</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Behold O, Master, the mighty army of the sons ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1376.264160</td>\n",
       "      <td>1331.519897</td>\n",
       "      <td>2368.946045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Present here are the mighty archers, peers or ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1003.821289</td>\n",
       "      <td>1086.919678</td>\n",
       "      <td>1332.740845</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arjuna's Vishada Yoga</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Dhrishtaketu, Chekitana, and the valiant king ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.724609</td>\n",
       "      <td>1457.762939</td>\n",
       "      <td>1932.723999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Title  Chapter  Verse  \\\n",
       "0  Arjuna's Vishada Yoga        1      1   \n",
       "1  Arjuna's Vishada Yoga        1      2   \n",
       "2  Arjuna's Vishada Yoga        1      3   \n",
       "3  Arjuna's Vishada Yoga        1      4   \n",
       "4  Arjuna's Vishada Yoga        1      5   \n",
       "\n",
       "                                 English Translation           a1  \\\n",
       "0  Dhrtarashtra asked of Sanjaya: O SANJAYA, what...  1082.754639   \n",
       "1  Sanjaya explained: Now seeing that the army of...     0.000000   \n",
       "2  Behold O, Master, the mighty army of the sons ...     0.000000   \n",
       "3  Present here are the mighty archers, peers or ...     0.000000   \n",
       "4  Dhrishtaketu, Chekitana, and the valiant king ...     0.000000   \n",
       "\n",
       "            a2           a3           a4   a5  \n",
       "0   896.849670   642.481567     0.000000  0.0  \n",
       "1   723.449585  1098.451416   427.873535  0.0  \n",
       "2  1376.264160  1331.519897  2368.946045  0.0  \n",
       "3  1003.821289  1086.919678  1332.740845  0.0  \n",
       "4  1400.724609  1457.762939  1932.723999  0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhagvadgita_final = pd.concat([bhagvadgita, encodf], axis=1)\n",
    "bhagvadgita_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhagvadgita_final.to_csv('../dataset/bhagvadgita_encoded.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c84e5e0094cb720dcb83feadb93e4b30c9501c23e2a5cb9df6091d8c79ac21d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
